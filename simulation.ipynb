{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head-to-Head Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A random agent. Random agents is for running toy examples on the card games\"\"\"\n",
    "\n",
    "class RandomAgent(object):\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        ''' Initilize the random agent\n",
    "\n",
    "        Args:\n",
    "            num_actions (int): The size of the ouput action space\n",
    "        '''\n",
    "        self.use_raw = False\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "\n",
    "    \"\"\"Predict the action given the curent state in gerenerating training data.\"\"\"\n",
    "    @staticmethod\n",
    "    def step(state):\n",
    "        '''\n",
    "        Args:\n",
    "            state (dict): An dictionary that represents the current state\n",
    "        Returns:\n",
    "            action (int): The action predicted (randomly chosen) by the random agent\n",
    "        '''\n",
    "        return np.random.choice(list(state['legal_actions'].keys()))\n",
    "\n",
    "    \"\"\"Predict the action given the current state for evaluation.\n",
    "            Since the random agents are not trained. This function is equivalent to step function\"\"\"\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        ''' \n",
    "        Args:\n",
    "            state (dict): An dictionary that represents the current state\n",
    "\n",
    "        Returns:\n",
    "            action (int): The action predicted (randomly chosen) by the random agent\n",
    "            probs (list): The list of action probabilities\n",
    "        '''\n",
    "        probs = [0 for _ in range(self.num_actions)]\n",
    "        for i in state['legal_actions']:\n",
    "            probs[i] = 1/len(state['legal_actions'])\n",
    "\n",
    "        info = {}\n",
    "        info['probs'] = {state['raw_legal_actions'][i]: probs[list(state['legal_actions'].keys())[i]] for i in range(len(state['legal_actions']))}\n",
    "\n",
    "        return self.step(state), info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFR agent against Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leduc_test import LeducholdemEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make environment\n",
    "env = LeducholdemEnv(Env)\n",
    "random_agent = RandomAgent(env.num_actions)\n",
    "cfr_agent = models.load('leduc-holdem-cfr').agents[0]\n",
    "env.set_agents([\n",
    "    random_agent,\n",
    "    cfr_agent,\n",
    "])\n",
    "\n",
    "print(\">> Leduc Hold'em pre-trained model\")\n",
    "\n",
    "iteration = 10000\n",
    "\n",
    "EA_wealth = []\n",
    "EA_wins = []\n",
    "EA_cumulative_wealth = 0\n",
    "EA_cumulative_wins = 0\n",
    "\n",
    "for i in range(iteration):\n",
    "    while (True):\n",
    "        print(\">> Start a new game\")\n",
    "\n",
    "        trajectories, payoffs = env.run(is_training=False)\n",
    "        # If the human does not take the final action, we need to\n",
    "        # print other players action\n",
    "        final_state = trajectories[0][-1]\n",
    "        action_record = final_state['action_record']\n",
    "        state = final_state['raw_obs']\n",
    "        _action_list = []\n",
    "        for i in range(1, len(action_record)+1):\n",
    "            if action_record[-i][0] == state['current_player']:\n",
    "                break\n",
    "            _action_list.insert(0, action_record[-i])\n",
    "        \n",
    "        EA_cumulative_wealth += payoffs[0]\n",
    "        \n",
    "        # Record the cumulative wealth\n",
    "        EA_wealth.append(EA_cumulative_wealth)\n",
    "\n",
    "        if payoffs[0] > 0:\n",
    "            EA_cumulative_wins += 1\n",
    "        \n",
    "        EA_wins.append(EA_cumulative_wins)\n",
    "\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(EA_wealth)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cumulative Wealth')\n",
    "plt.title('Player A Cumulative Wealth over Iterations')\n",
    "\n",
    "# Plotting the cumulative wins over iterations\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(EA_wins)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cumulative Wins')\n",
    "plt.title('Player A Cumulative Wins over Iterations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-average CFR agent against Ensemble-average CFR agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonICMgame_env = LeducholdemEnv(Env)\n",
    "TA_cfr_agent = models.load('leduc-holdem-cfr').agents[0]\n",
    "EA_cfr_agent = models.load('leduc-holdem-cfr').agents[0]\n",
    "nonICMgame_env.set_agents([\n",
    "    EA_cfr_agent,\n",
    "    TA_cfr_agent,\n",
    "])\n",
    "\n",
    "print(\">> Leduc Hold'em pre-trained model\")\n",
    "\n",
    "iteration = 10000\n",
    "\n",
    "TA_wealth = []\n",
    "TA_wins = []\n",
    "TA_cumulative_wealth = 0\n",
    "TA_cumulative_wins = 0\n",
    "\n",
    "for i in range(iteration):\n",
    "    while (True):\n",
    "        print(\">> Start a new game\")\n",
    "\n",
    "        trajectories, payoffs = nonICMgame_env.run(is_training=False)\n",
    "        # If the human does not take the final action, we need to\n",
    "        # print other players action\n",
    "        final_state = trajectories[0][-1]\n",
    "        action_record = final_state['action_record']\n",
    "        state = final_state['raw_obs']\n",
    "        _action_list = []\n",
    "        for i in range(1, len(action_record)+1):\n",
    "            if action_record[-i][0] == state['current_player']:\n",
    "                break\n",
    "            _action_list.insert(0, action_record[-i])\n",
    "        \n",
    "        TA_cumulative_wealth += payoffs[0]\n",
    "        \n",
    "        # Record the cumulative wealth\n",
    "        TA_wealth.append(TA_cumulative_wealth)\n",
    "\n",
    "        if payoffs[0] > 0:\n",
    "            TA_cumulative_wins += 1\n",
    "        \n",
    "        TA_wins.append(TA_cumulative_wins)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(TA_wealth)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cumulative Wealth')\n",
    "plt.title('Player A Cumulative Wealth over Iterations')\n",
    "\n",
    "# Plotting the cumulative wins over iterations\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plott(TA_wins)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cumulative Wins')\n",
    "plt.title('Player A Cumulative Wins over Iterations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
